[{"path":"https://hadley.github.io/elmer/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 elmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Posit Software, PBC. Copyright holder, funder.","code":""},{"path":"https://hadley.github.io/elmer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H (2024). elmer: Call LLM APIs R. R package version 0.0.0.9000, https://hadley.github.io/elmer/, https://github.com/hadley/elmer.","code":"@Manual{,   title = {elmer: Call LLM APIs from R},   author = {Hadley Wickham},   year = {2024},   note = {R package version 0.0.0.9000, https://hadley.github.io/elmer/},   url = {https://github.com/hadley/elmer}, }"},{"path":"https://hadley.github.io/elmer/index.html","id":"elmer-","dir":"","previous_headings":"","what":"Call LLM APIs from R","title":"Call LLM APIs from R","text":"goal elmer provide user friendly wrapper common APIs calling llm’s. Major design goals include support streaming making easy register call R functions.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Call LLM APIs from R","text":"can install development version elmer GitHub :","code":"# install.packages(\"pak\") pak::pak(\"hadley/elmer\")"},{"path":"https://hadley.github.io/elmer/index.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Call LLM APIs from R","text":"use elmer, need OpenAI API key. can get one developer console. save value OPENAI_API_KEY environment variable ~/.Renviron (easy way open file call usethis::edit_r_environ()).","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"using-elmer","dir":"","previous_headings":"","what":"Using elmer","title":"Call LLM APIs from R","text":"chat elmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful: retain context conversation, new query can build previous ones. true regardless various ways chatting use.","code":"library(elmer)  chat <- new_chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\",   echo = TRUE )"},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using elmer","what":"Interactive chat console","title":"Call LLM APIs from R","text":"interactive, least programmatic way using elmer chat directly R console. chat console useful quickly exploring capabilities model, especially ’ve customized chat object tool integrations (see ). , keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist even exit back R prompt.","code":"chat_console(chat) ╔════════════════════════════════════════════════════════╗ ║  Entering chat console. Use \"\"\" for multi-line input.  ║ ║  Press Ctrl+C to quit.                                 ║ ╚════════════════════════════════════════════════════════╝ >>> Who were the original creators of R? R was originally created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand.  >>> When was that? R was initially released in 1995. Development began a few years prior to that, in the early 1990s."},{"path":"https://hadley.github.io/elmer/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using elmer","what":"Interactive method call","title":"Call LLM APIs from R","text":"second interactive way chat using elmer call chat() method. initialize chat object echo = TRUE, , chat method streams response console arrives. entire response received, returned character vector (invisibly, ’s printed twice). mode useful want see response arrives, don’t want enter chat console.","code":"chat$chat(\"What preceding languages most influenced R?\") R was primarily influenced by the S programming language, particularly S-PLUS. Other languages that had an impact include Scheme and various data analysis languages."},{"path":"https://hadley.github.io/elmer/index.html","id":"vision-image-input","dir":"","previous_headings":"Using elmer > Interactive method call","what":"Vision (image input)","title":"Call LLM APIs from R","text":"want ask question image, can pass one additional input arguments using content_image_file() /content_image_url(). content_image_url function takes URL image file sends URL directly API. content_image_file function takes path local image file encodes base64 string send API. Note default, content_image_file automatically resizes image fit within 512x512 pixels; set resize parameter \"high\" higher resolution needed.","code":"chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) The logo of R features a stylized letter \"R\" in blue, enclosed in an oval shape that resembles the letter \"O,\" signifying the programming language's name. The design conveys a modern and professional look, reflecting its use in statistical computing and data analysis. The blue color often represents trust and reliability, which aligns with R's role in data science."},{"path":"https://hadley.github.io/elmer/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using elmer","what":"Programmatic chat","title":"Call LLM APIs from R","text":"don’t want see response arrives, can turn echoing leaving echo = TRUE argument new_chat_openai(). mode useful programming using elmer, result either intended human consumption want process response displaying .","code":"chat <- new_chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\" ) chat$chat(\"Is R a functional programming language?\") [1] \"Yes, R supports functional programming concepts. It allows functions to be first-class objects, supports higher-order functions, and encourages the use of functions as core components of code. However, it also supports procedural and object-oriented programming styles.\""},{"path":"https://hadley.github.io/elmer/index.html","id":"streaming-results","dir":"","previous_headings":"Using elmer","what":"Streaming results","title":"Call LLM APIs from R","text":"chat() method return results entire response received. (can print streaming results console, returns result response complete.) want process response arrives, can use stream() method. may useful want display response realtime, somewhere R console (like writing file, HTTP response, Shiny chat window); want manipulate response displaying , without giving immediacy streaming. stream() method returns generator coro package, can loop process response arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) R IS COMMONLY USED FOR:  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. 2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE GGPLOT2. 3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. 4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND RANDOMFOREST. 5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. 6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. 7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. 8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING.  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://hadley.github.io/elmer/index.html","id":"async-usage-advanced","dir":"","previous_headings":"","what":"Async usage (Advanced)","title":"Call LLM APIs from R","text":"elmer also supports async usage, useful want run multiple chat sessions concurrently. primarily useful Shiny applications, using methods described block Shiny app users duration response. use async chat, instead chat()/stream(), call chat_async()/stream_async(). _async variants take arguments construction, return promises instead actual response. Remember chat objects stateful, maintaining conversation history interact . Note means doesn’t make sense issue multiple chat/stream operations chat object concurrently, conversation history become corrupted interleaved conversation fragments. need run multiple chat sessions concurrently, create multiple chat objects.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"asynchronous-chat","dir":"","previous_headings":"Async usage (Advanced)","what":"Asynchronous chat","title":"Call LLM APIs from R","text":"asynchronous, non-streaming chat, use chat() method , handle result promise instead string. TODO: Shiny example","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://hadley.github.io/elmer/index.html","id":"asynchronous-streaming","dir":"","previous_headings":"Async usage (Advanced)","what":"Asynchronous streaming","title":"Call LLM APIs from R","text":"asynchronous streaming, use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced, require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, can simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() <Promise [pending]> > R IS COMMONLY USED FOR:  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. 2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. 3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. 4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. 5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. 6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. 7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. 8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://hadley.github.io/elmer/index.html","id":"tool-calling-aka-function-calling","dir":"","previous_headings":"","what":"Tool calling (a.k.a. function calling)","title":"Call LLM APIs from R","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":""},{"path":"https://hadley.github.io/elmer/index.html","id":"motivating-example","dir":"","previous_headings":"Tool calling (a.k.a. function calling)","what":"Motivating example","title":"Call LLM APIs from R","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Unfortunately, example run September 18, 2024. Let’s give chat model ability determine current time try .","code":"chat <- new_chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that was from the current year of 2023, we can calculate the difference in years, months, and days.  From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you would add the additional time since then. If it is before, you would consider slightly less than 54 years.  As of right now, can you confirm the current date so we can calculate the precise duration?"},{"path":"https://hadley.github.io/elmer/index.html","id":"defining-a-tool-function","dir":"","previous_headings":"Tool calling (a.k.a. function calling)","what":"Defining a tool function","title":"Call LLM APIs from R","text":"first thing ’ll define R function returns current time. tool. Note ’ve gone trouble creating roxygen2 comments. important step help model use tool correctly! Let’s test :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time() [1] \"2024-09-18 17:47:14 UTC\""},{"path":"https://hadley.github.io/elmer/index.html","id":"registering-tools","dir":"","previous_headings":"Tool calling (a.k.a. function calling)","what":"Registering tools","title":"Call LLM APIs from R","text":"Now need tell chat object get_current_time function. done using register_tool() method. fair amount code write, even simple function get_current_time. Fortunately, don’t write hand! generated register_tool call calling create_tool_metadata(get_current_time), printed code console. create_tool_metadata() works passing function’s signature documentation GPT-4o, asking generate register_tool call . Note create_tool_metadata() may create perfect results, must review generated code using . huge time-saver nonetheless, removes tedious boilerplate generation ’d otherwise.","code":"chat <- new_chat_openai(model = \"gpt-4o\")  chat$register_tool(   fun = get_current_time,   description = \"Gets the current time in the given time zone.\",   arguments = list(     tz = tool_arg(       type = \"string\",       description = \"The time zone to get the current time in. Defaults to `\\\"UTC\\\"`.\",       required = FALSE     )   ) )"},{"path":"https://hadley.github.io/elmer/index.html","id":"using-the-tool","dir":"","previous_headings":"Tool calling (a.k.a. function calling)","what":"Using the tool","title":"Call LLM APIs from R","text":"’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using elmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC.  To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19 UTC), we need to break it down.  1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years. 2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down:     - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is:      - 1 full month (August)      - 30 – 20 = 10 days of July      - 18 days of September until 17:47:19 UTC  So, in detail:    - 55 years    - 1 month    - 28 days    - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds  Time Total: - 55 years - 1 month - 28 days - 23 hours - 30 minutes - 19 seconds  This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon."},{"path":"https://hadley.github.io/elmer/index.html","id":"tool-limitations","dir":"","previous_headings":"Tool calling (a.k.a. function calling)","what":"Tool limitations","title":"Call LLM APIs from R","text":"Remember tool arguments come chat model, tool results returned chat model. means simple, jsonlite compatible data types can used inputs outputs. ’s highly recommended stick strings/character, numbers, booleans/logical, null, named unnamed lists types. can forget using functions, environments, external pointers, R6 classes, complex R objects arguments return values. Returning data frames seems work OK, although careful return much data, counts tokens (.e., count context window limit also cost money).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat object — Chat","title":"Chat object — Chat","text":"Chat object represents sequence messages user chat API. generally create object , instead call new_chat_openai() friends.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat object — Chat","text":"promise resolves string (probably Markdown).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Chat object — Chat","text":"system_prompt system prompt, , string.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Chat object — Chat","text":"Chat$new() Chat$messages() Chat$last_message() Chat$chat() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$clone()","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$new(provider, messages, seed = NULL, echo = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"provider provider object. messages unnamed list messages start chat (.e., continuing previous conversation). NULL zero-length list, conversation begins scratch. seed Optional integer seed ChatGPT uses try make output reproducible. echo TRUE, chat() method streams response stdout (also returning final response). Note effect stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-messages-","dir":"Reference","previous_headings":"","what":"Method messages()","title":"Chat object — Chat","text":"messages sent received far (optionally starting system prompt, ).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$messages(include_system_prompt = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"include_system_prompt Whether include system prompt messages (exists).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-last-message-","dir":"Reference","previous_headings":"","what":"Method last_message()","title":"Chat object — Chat","text":"last message returned assistant.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$last_message()"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Chat object — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"Chat object — Chat","text":"Submit input chatbot, receive promise resolves response .","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$chat_async(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"Chat object — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$stream(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"Chat object — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$stream_async(...)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"Chat object — Chat","text":"Register tool (R function) chatbot can use. chatbot decides use function,  elmer automatically call submit results back. (See create_tool_metadata() AI-enabled helper function can write register_tool call cases.)","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$register_tool(   fun,   name = NULL,   description,   arguments = list(),   strict = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"fun function invoked tool called. name name function. description detailed description function . Generally, information can provide , better. arguments named list arguments function accepts. named list objects created tool_arg(). strict argument definition strictly enforced? TRUE, enables Structured Output mode, comes number additional requirements.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Chat object — Chat","text":"objects class cloneable method.","code":""},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat object — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat object — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open an interactive chat application — chat_console","title":"Open an interactive chat application — chat_console","text":"chat_console() lets chat interactively console. chat_browser() lets chat interactively browser. Note functions mutate input chat object chat messages appended history.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open an interactive chat application — chat_console","text":"","code":"chat_console(chat, quiet = FALSE)  chat_browser(chat, quiet = FALSE)"},{"path":"https://hadley.github.io/elmer/reference/chat_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open an interactive chat application — chat_console","text":"chat chat object created new_chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://hadley.github.io/elmer/reference/chat_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open an interactive chat application — chat_console","text":"(Invisibly) input chat.","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode image content for chat input — content_image_url","title":"Encode image content for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode image content for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode image content for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode image content for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://hadley.github.io/elmer/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode image content for chat input — content_image_url","text":"","code":"if (FALSE) { # elmer:::openai_key_exists() chat <- new_chat_openai(echo = TRUE) chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"elmer\")) )  DONTSHOW({dev.control('enable')}) plot(waiting ~ eruptions, data = faithful) chat <- new_chat_openai(echo = TRUE) chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) }"},{"path":"https://hadley.github.io/elmer/reference/create_tool_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_metadata","title":"Create metadata for a tool — create_tool_metadata","text":"order use function tool chat, need craft right metadata pass register_tool(). function helps documented functions extracting function's R documentation creating register_tool call , using LLM. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful Roxygen comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_metadata","text":"","code":"create_tool_metadata(   topic,   model = \"gpt-4o\",   echo = interactive(),   verbose = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/create_tool_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_metadata","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. model OpenAI model use generating metadata. Defaults \"gpt-4o\", highly recommended \"gpt-4o-mini\". echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_metadata","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://hadley.github.io/elmer/reference/create_tool_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_metadata","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_metadata(rnorm)   create_tool_metadata(stats::rnorm)   create_tool_metadata(\"rnorm\") } # }"},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"elmer: Call LLM APIs from R — elmer-package","title":"elmer: Call LLM APIs from R — elmer-package","text":"consistent interface calling LLM APIs. Includes support streaming.","code":""},{"path":[]},{"path":"https://hadley.github.io/elmer/reference/elmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"elmer: Call LLM APIs from R — elmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co contributors: Posit Software, PBC [copyright holder, funder]","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with models in the GitHub model marketplace — new_chat_github","title":"Chat with models in the GitHub model marketplace — new_chat_github","text":"assumes accepted beta access program https://github.com/marketplace/models. function lightweight wrapper around new_chat_openai() defaults tweaked GitHub model marketplace.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with models in the GitHub model marketplace — new_chat_github","text":"","code":"new_chat_github(   system_prompt = NULL,   messages = NULL,   base_url = \"https://models.inference.ai.azure.com/\",   api_key = github_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with models in the GitHub model marketplace — new_chat_github","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments passed every chat API call. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with models from Groq — new_chat_groq","title":"Chat with models from Groq — new_chat_groq","text":"Sign https://groq.com. function lightweight wrapper around new_chat_openai() defaults tweaked groq.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with models from Groq — new_chat_groq","text":"","code":"new_chat_groq(   system_prompt = NULL,   messages = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with models from Groq — new_chat_groq","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments passed every chat API call. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to a local ollama instances — new_chat_ollama","title":"Connect to a local ollama instances — new_chat_ollama","text":"Download install ollama can chat R new_chat_ollama(). install additional models, use ollama command line, e.g. ollama pull llama3.1 ollama pull gemma2. function lightweight wrapper around new_chat_openai() defaults tweaked ollama.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to a local ollama instances — new_chat_ollama","text":"","code":"new_chat_ollama(   system_prompt = NULL,   messages = NULL,   base_url = \"http://localhost:11434/v1\",   model,   seed = NULL,   api_args = list(),   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to a local ollama instances — new_chat_ollama","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments passed every chat API call. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"function returns Chat object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (aka R function), also takes care tool loop.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"new_chat_openai(   system_prompt = NULL,   messages = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments passed every chat API call. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"Chat object.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to an OpenAI compatible endpoint — new_chat_openai","text":"","code":"if (FALSE) { # elmer:::openai_key_exists() chat <- new_chat_openai() chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \")  chat <- new_chat_openai() chat$register_tool(   fun = rnorm,   name = \"rnorm\",   description = \"Drawn numbers from a random normal distribution\",   arguments = list(     n = tool_arg(       type = \"integer\",       description = \"The number of observations. Must be a positive integer.\"     ),     mean = tool_arg(       type = \"number\",       description = \"The mean value of the distribution.\"     ),     sd = tool_arg(       type = \"number\",       description = \"The standard deviation of the distribution. Must be a non-negative number.\"     )   ) ) chat$chat(\"   Give me five numbers from a random normal distribution.   Briefly explain your work. \") }"},{"path":"https://hadley.github.io/elmer/reference/new_chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with models from perplexity.ai — new_chat_perplexity","title":"Chat with models from perplexity.ai — new_chat_perplexity","text":"Sign https://www.perplexity.ai. function lightweight wrapper around new_chat_openai() defaults tweaked groq.","code":""},{"path":"https://hadley.github.io/elmer/reference/new_chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with models from perplexity.ai — new_chat_perplexity","text":"","code":"new_chat_perplexity(   system_prompt = NULL,   messages = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = FALSE )"},{"path":"https://hadley.github.io/elmer/reference/new_chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with models from perplexity.ai — new_chat_perplexity","text":"system_prompt system prompt set behavior assistant. messages list messages start chat (.e., continuing previous conversation). provided, conversation begins scratch. provide non-NULL values messages system_prompt. message list named list least role (usually system, user, assistant, tool also possible). Normally also content field, string. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments passed every chat API call. echo TRUE, chat() method streams response stdout default. (Note effect stream(), chat_async(), stream_async() methods.)","code":""},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":null,"dir":"Reference","previous_headings":"","what":"Define arguments for a tool — tool_arg","title":"Define arguments for a tool — tool_arg","text":"Define arguments tool","code":""},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define arguments for a tool — tool_arg","text":"","code":"tool_arg(type, description, ..., required = TRUE)"},{"path":"https://hadley.github.io/elmer/reference/tool_arg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define arguments for a tool — tool_arg","text":"type Argument type (\"null\", \"boolean\", \"object\", \"array\", \"number\", \"string\"). description Description argument free text. ... Additional JSON Schema properties (e.g. properties, enum, pattern). required argument required?","code":""}]
